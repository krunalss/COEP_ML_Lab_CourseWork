# -*- coding: utf-8 -*-
"""ML_lab_assign3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WJbQ4ajyaRCUu-44i3NftEMK0tQ6QMod
"""

# 1. Import libraries

import numpy as np
import pandas as pd

from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    ConfusionMatrixDisplay,
    classification_report
)

import matplotlib.pyplot as plt
import seaborn as sns

# Plot styling
sns.set(style="whitegrid", context="notebook")
plt.rcParams["figure.figsize"] = (8, 6)
np.random.seed(42)

# 2. Loading Breast Cancer Wisconsin dataset and create DataFrame

data = load_breast_cancer()

# Features and target
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target, name="target")

# Combine into single DataFrame for EDA
df = X.copy()
df["target"] = y

print("Shape of feature matrix:", X.shape)
print("Shape of target vector:", y.shape)
print("\nTarget classes:", dict(zip(data.target_names, np.unique(y))))

# feature names + target

print("All feature names:")
for i, col in enumerate(data.feature_names, start=1):
    print(f"{i:2d}. {col}")

print("\nTarget name:", data.target_names)

# Subset of features

subset_features = [
    "mean radius",
    "mean texture",
    "mean perimeter",
    "mean area",
    "mean smoothness",
    "worst radius",
    "worst perimeter",
    "worst area"
]

print("Subset of features used in this experiment:")
for f in subset_features:
    print("-", f)

X_subset = X[subset_features]
X_subset.head()

#Checking missing values

print("Missing values per column:\n")
print(df.isna().sum())

# Train-test spliting on subset features

X_train, X_test, y_train, y_test = train_test_split(
    X_subset, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print("Train shape:", X_train.shape)
print("Test shape :", X_test.shape)

#Function to train & evaluate models

def evaluate_model(name, model, X_train, X_test, y_train, y_test, show_cm=True):
    """
    Fits the model, evaluates on test set, prints metrics, and optionally plots confusion matrix.
    Returns a dict of metric scores.
    """
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    acc  = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    rec  = recall_score(y_test, y_pred)
    f1   = f1_score(y_test, y_pred)

    print(f"=== {name} ===")
    print(f"Accuracy : {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall   : {rec:.4f}")
    print(f"F1-score : {f1:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=data.target_names))

    if show_cm:
        cm = confusion_matrix(y_test, y_pred)
        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)
        disp.plot(cmap="Blues")
        plt.title(f"Confusion Matrix - {name}")
        plt.show()

    return {
        "model": name,
        "accuracy": acc,
        "precision": prec,
        "recall": rec,
        "f1_score": f1
    }

#Logistic Regression with standard scaling

log_reg_clf = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", LogisticRegression(max_iter=1000, random_state=42))
])

results = []  # to collect all metrics
results.append(
    evaluate_model("Logistic Regression", log_reg_clf, X_train, X_test, y_train, y_test)
)

# KNN classifier with scaling

knn_clf = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", KNeighborsClassifier(n_neighbors=5))
])

results.append(
    evaluate_model("K-Nearest Neighbors", knn_clf, X_train, X_test, y_train, y_test)
)

# Decision Tree classifier (no scaling needed)

dt_clf = DecisionTreeClassifier(
    criterion="gini",
    max_depth=None,  # let the tree grow; can tune later
    random_state=42
)

results.append(
    evaluate_model("Decision Tree", dt_clf, X_train, X_test, y_train, y_test)
)

# SVC with RBF kernel + scaling

svc_clf = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", SVC(kernel="rbf", random_state=42))
])

results.append(
    evaluate_model("SVC (RBF Kernel)", svc_clf, X_train, X_test, y_train, y_test)
)

# Comparing all models

results_df = pd.DataFrame(results).set_index("model")
results_df.sort_values("accuracy", ascending=False)

# Visualizing model performance (accuracy, precision, recall, F1)

ax = results_df[["accuracy", "precision", "recall", "f1_score"]].plot(
    kind="bar",
    figsize=(10, 6)
)
plt.title("Model Comparison on Breast Cancer Dataset")
plt.ylabel("Score")
plt.ylim(0, 1.05)
plt.xticks(rotation=20, ha="right")
plt.legend(loc="lower right")
plt.tight_layout()
plt.show()

#Identify best model (highest accuracy, i.e., least classification error)

best_model_name = results_df["accuracy"].idxmax()
best_accuracy = results_df["accuracy"].max()

print(f"Best model based on accuracy: {best_model_name}")
print(f"Best accuracy: {best_accuracy:.4f} (Error = {1 - best_accuracy:.4f})")

#Correlation matrix of all numerical features

corr = X.corr()

plt.figure(figsize=(14, 12))
sns.heatmap(
    corr,
    cmap="coolwarm",
    linewidths=0.5,
    square=True,
    cbar_kws={"shrink": 0.7}
)
plt.title("Correlation Matrix - Breast Cancer Features")
plt.tight_layout()
plt.show()