# -*- coding: utf-8 -*-
"""ML_lab_assign4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V-KaA5Bc9NX5YO95QBUjRvxp7fX24yTr
"""

# 1. Loading Libraries & Dataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

# Load dataset
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Series(iris.target)

X.head(), y.head()

# 2. Data Preprocessing

# Standardization
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

X_train[:5]

#logistic Regression

log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)

y_pred_lr = log_reg.predict(X_test)

print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))

# knn
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

y_pred_knn = knn.predict(X_test)

print("KNN Accuracy:", accuracy_score(y_test, y_pred_knn))
print(classification_report(y_test, y_pred_knn))

#Decision Tree
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

y_pred_dt = dt.predict(X_test)

print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dt))
print(classification_report(y_test, y_pred_dt))

#SVC
svc = SVC(kernel="rbf")
svc.fit(X_train, y_train)

y_pred_svc = svc.predict(X_test)

print("SVC Accuracy:", accuracy_score(y_test, y_pred_svc))
print(classification_report(y_test, y_pred_svc))

accuracy_scores = {
    "Logistic Regression": accuracy_score(y_test, y_pred_lr),
    "KNN": accuracy_score(y_test, y_pred_knn),
    "Decision Tree": accuracy_score(y_test, y_pred_dt),
    "SVC": accuracy_score(y_test, y_pred_svc),
}

accuracy_scores

#Confusion matrics
models = {
    "Logistic Regression": y_pred_lr,
    "KNN": y_pred_knn,
    "Decision Tree": y_pred_dt,
    "SVC": y_pred_svc,
}

plt.figure(figsize=(12, 10))

for i, (name, preds) in enumerate(models.items(), 1):
    plt.subplot(2, 2, i)
    sns.heatmap(confusion_matrix(y_test, preds), annot=True, cmap="Blues", fmt="d")
    plt.title(f"{name} - Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")

plt.tight_layout()
plt.show()

#Correlation Matrix

plt.figure(figsize=(8, 6))
sns.heatmap(X.corr(), annot=True, cmap="coolwarm")
plt.title("Correlation Matrix of Iris Dataset")
plt.show()

best_model = max(accuracy_scores, key=accuracy_scores.get)
best_accuracy = accuracy_scores[best_model]

print(f"Best model: {best_model} with accuracy {best_accuracy:.4f}")

from sklearn.model_selection import GridSearchCV

# Hyperparameter tuning for SVC
param_grid = {
    "C": [0.1, 1, 10, 50, 100],
    "gamma": ["scale", "auto", 0.01, 0.001],
    "kernel": ["rbf", "poly", "sigmoid"]
}

svc_grid = GridSearchCV(
    SVC(),
    param_grid,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)

svc_grid.fit(X_train, y_train)

print("Best Parameters:", svc_grid.best_params_)
print("Best CV Accuracy:", svc_grid.best_score_)

# Predictions from tuned model
y_pred_svc_tuned = svc_grid.predict(X_test)

print("Tuned SVC Test Accuracy:", accuracy_score(y_test, y_pred_svc_tuned))

# Accuracy comparison bar chart (Base models + Tuned SVC)

model_names = [
    "Logistic Regression",
    "KNN",
    "Decision Tree",
    "SVC (Base)",
    "SVC (Tuned)"
]

accuracies = [
    accuracy_score(y_test, y_pred_lr),
    accuracy_score(y_test, y_pred_knn),
    accuracy_score(y_test, y_pred_dt),
    accuracy_score(y_test, y_pred_svc),
    accuracy_score(y_test, y_pred_svc_tuned)
]

plt.figure(figsize=(10, 6))
sns.barplot(x=model_names, y=accuracies)
plt.ylim(0, 1)
plt.title("Accuracy Comparison: Base Models vs Tuned SVC")
plt.ylabel("Accuracy")
plt.xticks(rotation=45)
plt.show()

# Zoomed-in bargraph for model accuracies

model_names = [
    "Logistic Regression",
    "KNN",
    "Decision Tree",
    "SVC (Base)",
    "SVC (Tuned)"
]

accuracies = [
    accuracy_score(y_test, y_pred_lr),
    accuracy_score(y_test, y_pred_knn),
    accuracy_score(y_test, y_pred_dt),
    accuracy_score(y_test, y_pred_svc),
    accuracy_score(y_test, y_pred_svc_tuned)
]

plt.figure(figsize=(10, 6))
sns.barplot(x=model_names, y=accuracies)

# ZOOM IN (adjust limits to your values)
plt.ylim(min(accuracies) - 0.02, 1.00)

plt.title("Zoomed Accuracy Comparison: Base Models vs Tuned SVC")
plt.ylabel("Accuracy")
plt.xlabel("Models")
plt.xticks(rotation=45)
plt.grid(axis="y", linestyle="--", alpha=0.4)

plt.show()