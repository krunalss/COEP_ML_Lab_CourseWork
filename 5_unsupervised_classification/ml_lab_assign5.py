# -*- coding: utf-8 -*-
"""ML_lab_assign5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rf794IMRZUPLwTf5fjo9n-lgWtjMWxu0
"""

#Import libraries and load the Digits dataset

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_digits
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, adjusted_rand_score


plt.style.use("default") # For nicer plots

# Load dataset
digits = load_digits()
X = digits.data
y = digits.target
print("Shape of X:", X.shape)
print("Shape of y:", y.shape)
print("Unique digits:", np.unique(y))

# Create a DataFrame and apply basic data preprocessing (scaling)

# Create DataFrame for easier analysis
feature_names = [f"pixel_{i}" for i in range(X.shape[1])]
df = pd.DataFrame(X, columns=feature_names)
df["label"] = y

# Check for missing values
print(df.isna().sum().sum(), "missing values in the dataset")

# Separate features and labels
X_features = df.drop("label", axis=1)

# Scale features (important for clustering, PCA)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_features)

print("Scaled feature shape:", X_scaled.shape)

# Correlation matrix (on scaled features) and visualization

# Compute correlation matrix
corr_matrix = np.corrcoef(X_scaled, rowvar=False)  # 64 x 64

plt.figure(figsize=(10, 8))
sns.heatmap(
    corr_matrix,
    cmap="coolwarm",
    xticklabels=False,
    yticklabels=False
)
plt.title("Correlation Matrix of Digit Pixels (Scaled Features)")
plt.tight_layout()
plt.show()

# Dimensionality reduction using PCA (for visualization)

# Reduce to 2 components for plotting
pca_2d = PCA(n_components=2, random_state=42)
X_pca_2d = pca_2d.fit_transform(X_scaled)

print("Explained variance ratio by 2 components:", pca_2d.explained_variance_ratio_)
print("Total explained variance (2D):", pca_2d.explained_variance_ratio_.sum())

# Visualize true labels in PCA 2D space
plt.figure(figsize=(8, 6))
scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y, s=15)
plt.legend(*scatter.legend_elements(), title="Digit")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.title("PCA (2D) Projection Colored by True Digit Labels")
plt.tight_layout()
plt.show()

# K-Means clustering on scaled data (unsupervised)

k = 10
# we know digits 0–9, but algorithm doesn't use labels during training
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
kmeans_labels = kmeans.fit_predict(X_scaled)

# Evaluation using silhouette score and ARI (uses labels only for assessment)
sil_kmeans = silhouette_score(X_scaled, kmeans_labels)
ari_kmeans = adjusted_rand_score(y, kmeans_labels)

print("K-Means Silhouette Score:", sil_kmeans)
print("K-Means Adjusted Rand Index (vs true labels):", ari_kmeans)

# Visualize clusters in PCA 2D space
plt.figure(figsize=(8, 6))
scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=kmeans_labels, s=15)
plt.legend(*scatter.legend_elements(), title="Cluster")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.title("K-Means Clusters in PCA (2D) Space")
plt.tight_layout()
plt.show()

# Visualize K-Means cluster centers as digit images

centers = kmeans.cluster_centers_  # shape: (10, 64)

fig, axes = plt.subplots(2, 5, figsize=(8, 4))
axes = axes.ravel()

for idx, ax in enumerate(axes):
    ax.imshow(centers[idx].reshape(8, 8), cmap="gray")
    ax.set_title(f"Cluster {idx}")
    ax.axis("off")

plt.suptitle("K-Means Cluster Centers as Digits")
plt.tight_layout()
plt.show()

# PCA + K-Means on PCA-reduced data

pca_10 = PCA(n_components=10, random_state=42)
X_pca_10 = pca_10.fit_transform(X_scaled)

kmeans_pca = KMeans(n_clusters=k, random_state=42, n_init=10)
kmeans_pca_labels = kmeans_pca.fit_predict(X_pca_10)

sil_kmeans_pca = silhouette_score(X_pca_10, kmeans_pca_labels)
ari_kmeans_pca = adjusted_rand_score(y, kmeans_pca_labels)

print("K-Means on 10D PCA Silhouette Score:", sil_kmeans_pca)
print("K-Means on 10D PCA ARI:", ari_kmeans_pca)

# Agglomerative Clustering

agg = AgglomerativeClustering(n_clusters=k, linkage="ward")
agg_labels = agg.fit_predict(X_scaled)

sil_agg = silhouette_score(X_scaled, agg_labels)
ari_agg = adjusted_rand_score(y, agg_labels)

print("Agglomerative Clustering Silhouette Score:", sil_agg)
print("Agglomerative Clustering ARI:", ari_agg)

# Visualize Agglomerative clusters in PCA 2D space
plt.figure(figsize=(8, 6))
scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=agg_labels, s=15)
plt.legend(*scatter.legend_elements(), title="Cluster")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.title("Agglomerative Clusters in PCA (2D) Space")
plt.tight_layout()
plt.show()

# Hyperparameter Tuning for K-Means (GridSearch-like for unsupervised models)

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import numpy as np
import pandas as pd

param_grid = {
    "n_clusters": [8, 9, 10, 11, 12],
    "init": ["k-means++", "random"],
    "n_init": [5, 10, 20]
}

best_score = -1
best_params = None
results = []

# Loop through all combinations
for k in param_grid["n_clusters"]:
    for init_method in param_grid["init"]:
        for ninit in param_grid["n_init"]:

            kmeans = KMeans(
                n_clusters=k,
                init=init_method,
                n_init=ninit,
                random_state=42
            )

            labels = kmeans.fit_predict(X_scaled)
            sil_score = silhouette_score(X_scaled, labels)

            results.append({
                "clusters": k,
                "init": init_method,
                "n_init": ninit,
                "silhouette_score": sil_score
            })

            if sil_score > best_score:
                best_score = sil_score
                best_params = {
                    "clusters": k,
                    "init": init_method,
                    "n_init": ninit
                }

# Convert results to DataFrame
results_df = pd.DataFrame(results)
print("Grid Search Results:")
display(results_df.sort_values(by="silhouette_score", ascending=False))

print("\nBest Parameters:", best_params)
print("Best Silhouette Score:", best_score)

# Ploting the tuned K-Means clusters using best parameters found

# Re-run PCA (2D) if not already done
from sklearn.decomposition import PCA
pca_2d = PCA(n_components=2, random_state=42)
X_pca_2d = pca_2d.fit_transform(X_scaled)


kmeans_tuned = KMeans(
    n_clusters=best_params["clusters"],
    init=best_params["init"],
    n_init=best_params["n_init"],
    random_state=42
)

tuned_labels = kmeans_tuned.fit_predict(X_scaled)

# Plot the tuned clusters
plt.figure(figsize=(8, 6))
scatter = plt.scatter(
    X_pca_2d[:, 0],
    X_pca_2d[:, 1],
    c=tuned_labels,
    cmap="tab10",
    s=18
)

plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.title(f"Tuned K-Means Clusters (k={best_params['clusters']}, init={best_params['init']})")
plt.legend(*scatter.legend_elements(), title="Cluster")
plt.tight_layout()
plt.show()

# Comparison of all above clustering methods

from sklearn.metrics import silhouette_score, adjusted_rand_score

# --- 1. Base K-Means metrics ---
sil_kmeans = silhouette_score(X_scaled, kmeans_labels)
ari_kmeans = adjusted_rand_score(y, kmeans_labels)

# --- 2. K-Means PCA (10D) metrics ---
sil_kmeans_pca = silhouette_score(X_pca_10, kmeans_pca_labels)
ari_kmeans_pca = adjusted_rand_score(y, kmeans_pca_labels)

# --- 3. Agglomerative metrics ---
sil_agg = silhouette_score(X_scaled, agg_labels)
ari_agg = adjusted_rand_score(y, agg_labels)

# --- 4. Tuned K-Means metrics ---
kmeans_tuned = KMeans(
    n_clusters=best_params["clusters"],
    init=best_params["init"],
    n_init=best_params["n_init"],
    random_state=42
)

tuned_labels = kmeans_tuned.fit_predict(X_scaled)

sil_tuned = silhouette_score(X_scaled, tuned_labels)
ari_tuned = adjusted_rand_score(y, tuned_labels)


# --- Create comparison DataFrame ---
results = pd.DataFrame({
    "Method": [
        "K-Means (Base)",
        "K-Means PCA (10D)",
        "Agglomerative",
        "K-Means (Tuned)"
    ],
    "Silhouette Score": [
        sil_kmeans,
        sil_kmeans_pca,
        sil_agg,
        sil_tuned
    ],
    "Adjusted Rand Index": [
        ari_kmeans,
        ari_kmeans_pca,
        ari_agg,
        ari_tuned
    ]
})

print("Comparison of Clustering Models:")
display(results)


# --- Bar Plot: Adjusted Rand Index ---
plt.figure(figsize=(9, 5))
sns.barplot(data=results, x="Method", y="Adjusted Rand Index")
plt.title("Comparison of Clustering Methods (ARI)")
plt.ylabel("Adjusted Rand Index")
plt.xticks(rotation=20)
plt.tight_layout()
plt.show()


# --- Bar Plot: Silhouette Score ---
plt.figure(figsize=(9, 5))
sns.barplot(data=results, x="Method", y="Silhouette Score")
plt.title("Comparison of Clustering Methods (Silhouette Score)")
plt.ylabel("Silhouette Score")
plt.xticks(rotation=20)
plt.tight_layout()
plt.show()

# Visualize PCA components as "eigen-digits" (extra PCA visualization)

# Fit PCA with more components for visualization
pca_full = PCA(n_components=16, random_state=42)
pca_full.fit(X_scaled)

components = pca_full.components_  # shape: (16, 64)

fig, axes = plt.subplots(4, 4, figsize=(6, 6))
axes = axes.ravel()

for i in range(16):
    axes[i].imshow(components[i].reshape(8, 8), cmap="gray")
    axes[i].set_title(f"PC {i+1}")
    axes[i].axis("off")

plt.suptitle("First 16 PCA Components (Eigen-digits)")
plt.tight_layout()
plt.show()

# Visualization of Tuned K-Means Cluster Centers

# Fit tuned K-Means again (if not already done)
kmeans_tuned = KMeans(
    n_clusters=best_params["clusters"],
    init=best_params["init"],
    n_init=best_params["n_init"],
    random_state=42
)

kmeans_tuned.fit(X_scaled)

# Cluster centers (64 features → 8x8 images)
tuned_centers = kmeans_tuned.cluster_centers_

# Plot centers as images
num_clusters = best_params["clusters"]
rows = (num_clusters // 5) + 1  # arrange nicely
cols = 5

plt.figure(figsize=(10, 6))
for i in range(num_clusters):
    plt.subplot(rows, cols, i + 1)
    plt.imshow(tuned_centers[i].reshape(8, 8), cmap="gray")
    plt.title(f"Cluster {i}")
    plt.axis("off")

plt.suptitle(f"Tuned K-Means Cluster Centers (k={best_params['clusters']})", fontsize=14)
plt.tight_layout()
plt.show()